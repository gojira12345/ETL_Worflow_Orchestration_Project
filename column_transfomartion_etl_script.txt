import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Script generated for node AWS Glue Data Catalog
AWSGlueDataCatalog_node1714383961210 = glueContext.create_dynamic_frame.from_catalog(database="raw_db", table_name="data_store", transformation_ctx="AWSGlueDataCatalog_node1714383961210")

# Script generated for node Change Schema
ChangeSchema_node1714384071133 = ApplyMapping.apply(frame=AWSGlueDataCatalog_node1714383961210, mappings=[("id", "long", "id", "int"), ("product", "string", "product", "string"), ("price", "long", "price_in_inr", "decimal"), ("quantity", "long", "quantity", "int"), ("last_updated", "string", "last_updated_at", "string")], transformation_ctx="ChangeSchema_node1714384071133")

# Script generated for node Amazon S3
AmazonS3_node1714384128811 = glueContext.getSink(path="s3://primary-bucket-1/raw_file_first_transformation/", connection_type="s3", updateBehavior="UPDATE_IN_DATABASE", partitionKeys=[], enableUpdateCatalog=True, transformation_ctx="AmazonS3_node1714384128811")
AmazonS3_node1714384128811.setCatalogInfo(catalogDatabase="transformation_one_db",catalogTableName="transformation_one_table")
AmazonS3_node1714384128811.setFormat("glueparquet", compression="uncompressed")
AmazonS3_node1714384128811.writeFrame(ChangeSchema_node1714384071133)
job.commit()
